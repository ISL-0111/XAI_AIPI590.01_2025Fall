{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ISL-0111/XAI_AIPI590.01_2025Fall/blob/main/Week6_Explainable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1211a001",
      "metadata": {
        "id": "1211a001"
      },
      "source": [
        "## Explainable Deep Learning\n",
        "\n",
        "**Name: Ilseop(Shawn) Lee**\n",
        "\n",
        "### Instructions\n",
        "In this assignment, you will work with pretrained deep learning models to investigate model explainability in computer vision. Your objective is to apply GradCAM and at least two of its variants to a meaningful image classification problem of your choice and analyze *how* and *why* the model makes its decisions.\n",
        "\n",
        "You are encouraged to select an image classification task that holds personal or societal significance. Potential areas include, but are not limited to: wildlife conservation, road safety, public health, environmental sustainability, or social impact. You may use an existing public dataset or a curated subset, and pretrained models such as ResNet-50 or Vision Transformers (ViT).\n",
        "\n",
        "\n",
        "**Tasks**\n",
        "Choose an image classification problem relevant to you (e.g., wildlife detection, object recognition in autonomous driving, or recycling classification).<br>\n",
        "Use a pretrained computer vision model (e.g., ResNet-50, ViT) for your classification task. Transfer learning is optional.<br>\n",
        "Apply Explainability Techniques:<br>\n",
        "- Implement GradCAM and at least two GradCAM variants\n",
        "- Apply these techniques to at least 5 images from your dataset\n",
        "- Generate and present visualizations showing what regions of the image the model is focusing on for its predictions.\n",
        "- Compare and contrast the attention maps generated by GradCAM and its variants.\n",
        "\n",
        "Reflection:\n",
        "- Discuss the visual cues the model attends to\n",
        "- Comment on any surprising or misleading behavior\n",
        "- Reflect on why model explainability is important in your selected application domain\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7aab0738",
      "metadata": {
        "id": "7aab0738"
      },
      "source": [
        "- **Rooftop Solar Energy System Detection in Cape Town**\n",
        "    - Task : Detect and classify Solar Energy Systems (Solar Panel, Water Heater, Pool Heater) from aerial imagery\n",
        "    - Purpose : Assess spatial adoption patterns of solar energy system and provide evicende to suppoert energy policy and planning in Cape Town\n",
        "\n",
        "*Note: This was my personal project('Energy Transition During Energy Crisis: Cape Town's Experience'). Therefore, I used the dataset and model that I had processed for that project*\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f393361c",
      "metadata": {
        "id": "f393361c"
      },
      "source": [
        "**Model and Data Preparation**\n",
        "- Pre-trained Segmentation Mdoel\n",
        "    - ResNext-50 encoder with Feature Pyramid Network Decoder\n",
        "    - Loaded from a best performing checkpoint in ckpt format\n",
        "\n",
        "- Dataset\n",
        "    - Aerial Imagery from Cape Town (Cropped to 320*320)\n",
        "\n",
        "- Classification Task\n",
        "    - Class 0: Background, Class 1: Solar_Panel, Class 2: Water_heater, Class 3: Pool_heater\n",
        "\n",
        "- Visualization\n",
        "    - For Clarity, Solar Panels = 'Green', Water heater = 'Red', and Pool heater = 'Blue'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb086f83",
      "metadata": {
        "id": "fb086f83",
        "outputId": "34597904-0c19-48c5-e646-bcb27fb7e2ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/miniconda3/envs/yolo_11env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and dataset are ready.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/miniconda3/envs/yolo_11env/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['ce_loss.weight']\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This script code was generated using GPT-5 on 2025-10-02 at 16:10 and and then modified to fit class, file paths.\n",
        "'''\n",
        "import os, cv2, torch, numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset as BaseDataset\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "import segmentation_models_pytorch as smp\n",
        "from torch.optim import lr_scheduler\n",
        "import copy\n",
        "\n",
        "CLASSES = [\"background\", \"Solar_Panel\", \"Water_heater\", \"Pool_heater\"]\n",
        "COLORS = {0:(0,0,0), 1:(0,255,0), 2:(255,0,0), 3:(0,0,255)}\n",
        "\n",
        "CKPT_PATH = \"/Users/ilseoplee/XAI_AIPI590.01_2025Fall/Week6_artifacts/pv-model-epoch=38-valid_avg_PV_iou=0.9572.ckpt\"\n",
        "IMG_DIR   = \"/Users/ilseoplee/XAI_AIPI590.01_2025Fall/Week6_artifacts/Test_Images_Prd\"\n",
        "OUT_BASE  = \"/Users/ilseoplee/XAI_AIPI590.01_2025Fall/Week6_artifacts\"\n",
        "\n",
        "class PVModel(pl.LightningModule):\n",
        "    def __init__(self, arch, encoder_name, in_channels, out_classes, **kwargs):\n",
        "        super().__init__()\n",
        "        self.model = smp.create_model(\n",
        "            arch, encoder_name=encoder_name, in_channels=in_channels, classes=out_classes, **kwargs\n",
        "        )\n",
        "        params = smp.encoders.get_preprocessing_params(encoder_name)\n",
        "        self.register_buffer(\"std\", torch.tensor(params[\"std\"]).view(1,3,1,1))\n",
        "        self.register_buffer(\"mean\", torch.tensor(params[\"mean\"]).view(1,3,1,1))\n",
        "    def forward(self, image):\n",
        "        image = (image - self.mean) / self.std\n",
        "        return self.model(image)\n",
        "\n",
        "def get_validation_augmentation():\n",
        "    return A.Compose([\n",
        "        A.PadIfNeeded(min_height=320, min_width=320, border_mode=0),\n",
        "        A.CenterCrop(height=320, width=320),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "class InferenceDataset(BaseDataset):\n",
        "    def __init__(self, image_dir, augmentation=None):\n",
        "        self.image_paths = [os.path.join(image_dir,f) for f in os.listdir(image_dir)]\n",
        "        self.augmentation = augmentation\n",
        "    def __len__(self): return len(self.image_paths)\n",
        "    def __getitem__(self,i):\n",
        "        img_path = self.image_paths[i]\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        if image.shape[:2] != (320,320):\n",
        "            image = cv2.resize(image,(320,320))\n",
        "        if self.augmentation:\n",
        "            sample = self.augmentation(image=image)\n",
        "            image = sample[\"image\"]\n",
        "        else:\n",
        "            image = torch.from_numpy(image.transpose(2,0,1)).float()\n",
        "        return image, str(img_path)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = PVModel.load_from_checkpoint(\n",
        "    CKPT_PATH,\n",
        "    arch=\"FPN\",\n",
        "    encoder_name=\"resnext50_32x4d\",\n",
        "    in_channels=3,\n",
        "    out_classes=len(CLASSES),\n",
        "    strict=False\n",
        ").to(device)\n",
        "model.eval()\n",
        "\n",
        "dataset = InferenceDataset(IMG_DIR, augmentation=get_validation_augmentation())\n",
        "loader  = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "target_layer = model.model.encoder.layer4[-1].conv3\n",
        "print(\"Model and dataset are ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "112006a3",
      "metadata": {
        "id": "112006a3"
      },
      "source": [
        "**Grad-CAM**\n",
        "- In this code, Grad-CAM is attached to the last convolutional layer of the ResNext-50 encoder. This makes it possible to visualize which spatial regions contributes to a given class prediction, represented as a heatmap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de70bb81",
      "metadata": {
        "id": "de70bb81",
        "outputId": "d222999f-1786-44d3-c3e0-8aa9ec6fb842"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grad-CAM visualizations saved to: /Users/ilseoplee/XAI_AIPI590.01_2025Fall/Week6_artifacts/gradcam_results\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This script code was generated using GPT-5 on 2025-10-02 at 17:23 and and then modified.\n",
        "'''\n",
        "\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients, self.activations = None, None\n",
        "        self.fwd_hook = target_layer.register_forward_hook(self.save_activation)\n",
        "        self.bwd_hook = target_layer.register_full_backward_hook(self.save_gradient)\n",
        "\n",
        "    def save_activation(self, module, input, output):\n",
        "        self.activations = output.detach()\n",
        "\n",
        "    def save_gradient(self, module, grad_input, grad_output):\n",
        "        self.gradients = grad_output[0].detach()\n",
        "\n",
        "    def __call__(self, input_tensor, target_class):\n",
        "        logits = self.model(input_tensor)\n",
        "        target = logits[:, target_class, :, :].mean()\n",
        "        self.model.zero_grad()\n",
        "        target.backward(retain_graph=True)\n",
        "\n",
        "        # Compute class activation map\n",
        "        weights = self.gradients.mean(dim=(2, 3), keepdim=True)\n",
        "        cam_map = (weights * self.activations).sum(dim=1, keepdim=True)\n",
        "        cam_map = torch.relu(cam_map).squeeze().cpu().numpy()\n",
        "        cam_map = (cam_map - cam_map.min()) / (cam_map.max() + 1e-8)\n",
        "        return cam_map, logits\n",
        "\n",
        "def overlay_heatmap(img, cam_map, alpha=0.5):\n",
        "    cam_map_resized = cv2.resize(cam_map, (img.shape[1], img.shape[0]))\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * cam_map_resized), cv2.COLORMAP_JET)\n",
        "    return cv2.addWeighted(img, 1 - alpha, heatmap, alpha, 0)\n",
        "\n",
        "# Output directory\n",
        "outdir_gc = os.path.join(OUT_BASE, \"gradcam_results\")\n",
        "os.makedirs(outdir_gc, exist_ok=True)\n",
        "\n",
        "gradcam = GradCAM(model, target_layer)\n",
        "\n",
        "# Generate Grad-CAM visualizations\n",
        "for imgs, img_paths in loader:\n",
        "    imgs = imgs.float().to(device)\n",
        "    img_path = img_paths if isinstance(img_paths, str) else img_paths[0]\n",
        "    orig = cv2.imread(img_path)\n",
        "\n",
        "    for cls_idx in [1, 2, 3]:\n",
        "        cam_map, _ = gradcam(imgs, cls_idx)\n",
        "        overlay = overlay_heatmap(orig, cam_map)\n",
        "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
        "        cv2.imwrite(os.path.join(outdir_gc, f\"{base}_gradcam_class{cls_idx}.png\"), overlay)\n",
        "\n",
        "print(f\"Grad-CAM visualizations saved to: {outdir_gc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfbc4b20",
      "metadata": {
        "id": "dfbc4b20"
      },
      "source": [
        "**Grad-CAM++**\n",
        "- Grad-CAM++ is applied to the last convolutional layer of the encoder. Unlike Grad-CAM, which highlights mainly the most dominant region by averaging gradients, Grad-CAM++ uses higher-order derivatives to assign more precise weights. This enables it to capture the contributions of multiple regions to the same class, producing more detailed and sharper heatmaps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b87b2c08",
      "metadata": {
        "id": "b87b2c08",
        "outputId": "4461fb62-834f-4541-b795-c3ccae64ddbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grad-CAM++ visualizations saved to: /Users/ilseoplee/XAI_AIPI590.01_2025Fall/Week6_artifacts/gradcam_results_plusplus\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This script code was generated using GPT-5 on 2025-10-02 at 17:53 and and then modified.\n",
        "'''\n",
        "\n",
        "# ---------------- Grad-CAM++ ----------------\n",
        "class GradCAMPlusPlus(GradCAM):\n",
        "    def __call__(self, input_tensor, target_class):\n",
        "        logits = self.model(input_tensor)\n",
        "        target = logits[:, target_class, :, :].mean()\n",
        "        self.model.zero_grad()\n",
        "        target.backward(retain_graph=True)\n",
        "\n",
        "        grads, acts = self.gradients, self.activations\n",
        "        grads2, grads3 = grads ** 2, grads ** 3\n",
        "\n",
        "        numerator = grads2\n",
        "        denominator = 2 * grads2 + (acts * grads3).sum(dim=(2, 3), keepdim=True) + 1e-8\n",
        "        alphas = numerator / denominator\n",
        "        weights = (alphas * torch.relu(grads)).sum(dim=(2, 3), keepdim=True)\n",
        "\n",
        "        # Compute Grad-CAM++ map\n",
        "        cam_map = (weights * acts).sum(dim=1, keepdim=True)\n",
        "        cam_map = torch.relu(cam_map).squeeze().cpu().numpy()\n",
        "        cam_map = (cam_map - cam_map.min()) / (cam_map.max() + 1e-8)\n",
        "        return cam_map, logits\n",
        "\n",
        "outdir_pp = os.path.join(OUT_BASE, \"gradcam_results_plusplus\")\n",
        "os.makedirs(outdir_pp, exist_ok=True)\n",
        "\n",
        "gradcam_pp = GradCAMPlusPlus(model, target_layer)\n",
        "\n",
        "# Generate Grad-CAM++ visualizations\n",
        "for imgs, img_paths in loader:\n",
        "    imgs = imgs.float().to(device)\n",
        "    img_path = img_paths if isinstance(img_paths, str) else img_paths[0]\n",
        "    orig = cv2.imread(img_path)\n",
        "\n",
        "    for cls_idx in [1, 2, 3]:\n",
        "        cam_map, _ = gradcam_pp(imgs, cls_idx)\n",
        "        overlay = overlay_heatmap(orig, cam_map)\n",
        "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
        "        cv2.imwrite(os.path.join(outdir_pp, f\"{base}_gradcampp_class{cls_idx}.png\"), overlay)\n",
        "\n",
        "print(f\"Grad-CAM++ visualizations saved to: {outdir_pp}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2522508",
      "metadata": {
        "id": "f2522508"
      },
      "source": [
        "**Guided_Grad_CAM**\n",
        "\n",
        "- Guided Grad-CAM combines Grad-CAM with Guided Backpropagation. Grad-CAM highlights the important regions for a prediction, while Guided Backprop reveals fine-grained edges and textures by allowing only positive contributions through ReLUs. By multiplying the two, Guided Grad-CAM produces sharp, pixel-level visualizations that preserve both where the model is looking and what fine details it relies on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c02193f2",
      "metadata": {
        "id": "c02193f2",
        "outputId": "ca893e5f-e04f-4bf7-f7b7-b7c73cd1d829"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Guided Grad-CAM visualizations saved to: /Users/ilseoplee/XAI_AIPI590.01_2025Fall/Week6_artifacts/gradcam_guided\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This script code was generated using GPT-5 on 2025-10-02 at 17:53 and and then modified.\n",
        "'''\n",
        "\n",
        "# 1) Define Guided Backpropagation ReLU\n",
        "class GuidedBackpropReLU(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        ctx.save_for_backward(input)\n",
        "        return torch.relu(input)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        (input,) = ctx.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        grad_input[input < 0] = 0\n",
        "        grad_input[grad_output < 0] = 0\n",
        "        return grad_input\n",
        "\n",
        "# 2) Replace all ReLUs in the model with GuidedBackpropReLU\n",
        "def replace_relu_with_guided(module):\n",
        "    for name, child in module.named_children():\n",
        "        if isinstance(child, nn.ReLU):\n",
        "            module._modules[name] = nn.ReLU(inplace=False)\n",
        "            module._modules[name].forward = lambda x: GuidedBackpropReLU.apply(x)\n",
        "        else:\n",
        "            replace_relu_with_guided(child)\n",
        "\n",
        "# 3) Create a copy of the model with guided ReLUs\n",
        "guided_model = copy.deepcopy(model)\n",
        "replace_relu_with_guided(guided_model)\n",
        "guided_model.eval()\n",
        "\n",
        "# 4) Use the same Grad-CAM instance\n",
        "gradcam_guided = GradCAM(model, target_layer)\n",
        "\n",
        "# 5) Output directory\n",
        "outdir_guided = os.path.join(OUT_BASE, \"gradcam_guided\")\n",
        "os.makedirs(outdir_guided, exist_ok=True)\n",
        "\n",
        "# 6) Run Guided Grad-CAM\n",
        "for imgs, img_paths in loader:\n",
        "    imgs = imgs.float().to(device)\n",
        "    img_path = img_paths if isinstance(img_paths, str) else img_paths[0]\n",
        "    orig = cv2.imread(img_path)\n",
        "\n",
        "    for cls_idx in [1, 2, 3]:\n",
        "        # Compute CAM\n",
        "        cam_map, _ = gradcam_guided(imgs, cls_idx)\n",
        "\n",
        "        # Guided Backpropagation\n",
        "        imgs.requires_grad = True\n",
        "        logits = guided_model(imgs)\n",
        "        target = logits[:, cls_idx, :, :].mean()\n",
        "        guided_model.zero_grad()\n",
        "        target.backward(retain_graph=True)\n",
        "\n",
        "        guided_grad = imgs.grad.detach().cpu().numpy()[0].transpose(1, 2, 0)\n",
        "        guided_grad = (guided_grad - guided_grad.min()) / (guided_grad.max() + 1e-8)\n",
        "\n",
        "        # Guided Grad-CAM = Guided Backprop * CAM\n",
        "        cam_resized = cv2.resize(cam_map, (guided_grad.shape[1], guided_grad.shape[0]))\n",
        "        guided_cam = guided_grad * cam_resized[..., np.newaxis]\n",
        "        guided_cam = (guided_cam - guided_cam.min()) / (guided_cam.max() + 1e-8)\n",
        "        guided_cam = np.uint8(255 * guided_cam)\n",
        "\n",
        "        # Save visualization\n",
        "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
        "        cv2.imwrite(os.path.join(outdir_guided, f\"{base}_guidedcam_class{cls_idx}.png\"), guided_cam)\n",
        "\n",
        "print(f\"Guided Grad-CAM visualizations saved to: {outdir_guided}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddbc59dd",
      "metadata": {
        "id": "ddbc59dd"
      },
      "source": [
        "**Reflection**\n",
        "\n",
        "- When I compared the different GradCAM methods, I noticed a clear difference in what they highlighted. Guided GradCAM returns the most precise pictures, clearly showing the details of the actual objects. The standard GradCAM usually showed similar areas, but GradCAM++ focused on surrounding regions rather than the actual objects.\n",
        "\n",
        "- This was confusing at first. I assumed the visualization would simply show the most relevant pixels for its correct prediction. But I learned that GradCAMs don't show where the models are most certain. Instead, it shows where the model's score is most sensitive. In other words, it highlights the areas that can still change the final decision as influential points.\n",
        "\n",
        "- In the visualization red areas indicate that the area(pixels) would significantly impact the model's confidence in the class. So, the model is highly sensitive to these spots. More interestingly, blue areas mean two things. (1) The model is ignoring that area completely, which is irrelevant for prediction or (2) The model is so certain about that area for classification tasks that gradient is not updated. That is so called, \"saturated\" region.\n",
        "\n",
        "- This explains why the actual object (the Ground Truth) was sometimes blue in a correct prediction. The last two water heater images. The model correctly identified them, but the heaters themselves were blue. This indicates that the model was already highly confident about the heater's location. That is, it was in a \"saturated\" state and didn't contribute to the sensitivity map.\n",
        "\n",
        "- The most insightful case was the ambiguous water heater that looked like a skylight in the last image. Instead of highlighting the water heater itself, the model responded to the small squared objects(e.g.,skylight) that share similar visual patterns with the target class. This shows that the red areas don't always mean that the model is struggling to interpret this area.\n",
        "\n",
        "- Even when predictions were classified as True Positives, I noticed that the model’s performance still varied across classes, especially struggling with water heaters. Through these visualizations, I was able to see whether the model’s decisions were truly based on detecting solar panels and to identify which objects caused confusion, such as water heaters and skylights. I believe such observations are valuable for refining the model's performance and improving its generalization to support more reliable decision-making in real-world energy applications."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This script code was generated using GPT-5 on 2025-10-04 at 01:02 and and then modified.\n",
        "'''\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "order = [1, 2, 3, 7, 6, 4, 5]\n",
        "base_url = \"https://github.com/ISL-0111/XAI_AIPI590.01_2025Fall/blob/main/Artifacts/Sample_{}.png?raw=true\"\n",
        "\n",
        "for i in order:\n",
        "    url = base_url.format(i)\n",
        "    display(HTML(f\"<img src='{url}' width='1200'>\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lNKHt2zPmRav",
        "outputId": "c6de30e3-cd45-4517-ca71-ff284ff07477"
      },
      "id": "lNKHt2zPmRav",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://github.com/ISL-0111/XAI_AIPI590.01_2025Fall/blob/main/Artifacts/Sample_1.png?raw=true' width='1200'>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://github.com/ISL-0111/XAI_AIPI590.01_2025Fall/blob/main/Artifacts/Sample_2.png?raw=true' width='1200'>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://github.com/ISL-0111/XAI_AIPI590.01_2025Fall/blob/main/Artifacts/Sample_3.png?raw=true' width='1200'>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://github.com/ISL-0111/XAI_AIPI590.01_2025Fall/blob/main/Artifacts/Sample_7.png?raw=true' width='1200'>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://github.com/ISL-0111/XAI_AIPI590.01_2025Fall/blob/main/Artifacts/Sample_6.png?raw=true' width='1200'>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://github.com/ISL-0111/XAI_AIPI590.01_2025Fall/blob/main/Artifacts/Sample_4.png?raw=true' width='1200'>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://github.com/ISL-0111/XAI_AIPI590.01_2025Fall/blob/main/Artifacts/Sample_5.png?raw=true' width='1200'>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "yolo_11env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}